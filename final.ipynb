{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\myick\\anaconda3\\envs\\zhou\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import evaluate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")  \n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [article for article in examples['article']]\n",
    "    targets = [summary for summary in examples['highlights']]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=150, truncation=True, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  \n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "train_data = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\myick\\anaconda3\\envs\\zhou\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          \n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    learning_rate=5e-5,              \n",
    "    per_device_train_batch_size=4,   \n",
    "    per_device_eval_batch_size=8,    \n",
    "    num_train_epochs=3,              \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir=\"./logs\",            \n",
    "    logging_steps=10,                \n",
    "    save_strategy=\"epoch\"           \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=train_data,  \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c644b6a2a554841bca40a0aff18ccf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.681, 'grad_norm': 4.6257853507995605, 'learning_rate': 4.976787372330548e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5356, 'grad_norm': 3.1615967750549316, 'learning_rate': 4.953574744661096e-05, 'epoch': 0.03}\n",
      "{'loss': 2.4958, 'grad_norm': 3.9571962356567383, 'learning_rate': 4.930362116991643e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3863, 'grad_norm': 3.3057658672332764, 'learning_rate': 4.9071494893221914e-05, 'epoch': 0.06}\n",
      "{'loss': 2.2961, 'grad_norm': 2.82592511177063, 'learning_rate': 4.883936861652739e-05, 'epoch': 0.07}\n",
      "{'loss': 2.2758, 'grad_norm': 3.300835132598877, 'learning_rate': 4.860724233983287e-05, 'epoch': 0.08}\n",
      "{'loss': 2.3751, 'grad_norm': 2.6961417198181152, 'learning_rate': 4.8375116063138345e-05, 'epoch': 0.1}\n",
      "{'loss': 2.144, 'grad_norm': 3.0563158988952637, 'learning_rate': 4.8142989786443826e-05, 'epoch': 0.11}\n",
      "{'loss': 2.3914, 'grad_norm': 3.7386505603790283, 'learning_rate': 4.79108635097493e-05, 'epoch': 0.13}\n",
      "{'loss': 2.3225, 'grad_norm': 3.1724650859832764, 'learning_rate': 4.767873723305478e-05, 'epoch': 0.14}\n",
      "{'loss': 2.2118, 'grad_norm': 2.3112425804138184, 'learning_rate': 4.744661095636026e-05, 'epoch': 0.15}\n",
      "{'loss': 2.3175, 'grad_norm': 2.9229588508605957, 'learning_rate': 4.721448467966574e-05, 'epoch': 0.17}\n",
      "{'loss': 2.3634, 'grad_norm': 2.8123764991760254, 'learning_rate': 4.698235840297121e-05, 'epoch': 0.18}\n",
      "{'loss': 2.2279, 'grad_norm': 6.22006368637085, 'learning_rate': 4.6750232126276694e-05, 'epoch': 0.19}\n",
      "{'loss': 2.266, 'grad_norm': 4.113021373748779, 'learning_rate': 4.6518105849582176e-05, 'epoch': 0.21}\n",
      "{'loss': 2.3157, 'grad_norm': 3.0069477558135986, 'learning_rate': 4.628597957288766e-05, 'epoch': 0.22}\n",
      "{'loss': 2.1341, 'grad_norm': 6.907096862792969, 'learning_rate': 4.605385329619313e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9925, 'grad_norm': 3.063687562942505, 'learning_rate': 4.582172701949861e-05, 'epoch': 0.25}\n",
      "{'loss': 2.1565, 'grad_norm': 2.695389986038208, 'learning_rate': 4.558960074280409e-05, 'epoch': 0.26}\n",
      "{'loss': 2.1063, 'grad_norm': 3.3808650970458984, 'learning_rate': 4.535747446610957e-05, 'epoch': 0.28}\n",
      "{'loss': 2.1699, 'grad_norm': 2.912640333175659, 'learning_rate': 4.5125348189415044e-05, 'epoch': 0.29}\n",
      "{'loss': 2.3377, 'grad_norm': 2.901578426361084, 'learning_rate': 4.4893221912720525e-05, 'epoch': 0.31}\n",
      "{'loss': 2.1711, 'grad_norm': 2.7211878299713135, 'learning_rate': 4.4661095636026e-05, 'epoch': 0.32}\n",
      "{'loss': 2.1079, 'grad_norm': 3.1929984092712402, 'learning_rate': 4.442896935933148e-05, 'epoch': 0.33}\n",
      "{'loss': 2.1376, 'grad_norm': 2.9461300373077393, 'learning_rate': 4.4196843082636956e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2142, 'grad_norm': 2.802509307861328, 'learning_rate': 4.396471680594244e-05, 'epoch': 0.36}\n",
      "{'loss': 1.9582, 'grad_norm': 3.1216139793395996, 'learning_rate': 4.373259052924791e-05, 'epoch': 0.38}\n",
      "{'loss': 2.139, 'grad_norm': 3.2219176292419434, 'learning_rate': 4.350046425255339e-05, 'epoch': 0.39}\n",
      "{'loss': 2.2725, 'grad_norm': 3.094008207321167, 'learning_rate': 4.326833797585887e-05, 'epoch': 0.4}\n",
      "{'loss': 2.3065, 'grad_norm': 2.90136981010437, 'learning_rate': 4.303621169916435e-05, 'epoch': 0.42}\n",
      "{'loss': 2.3338, 'grad_norm': 2.697258710861206, 'learning_rate': 4.2804085422469823e-05, 'epoch': 0.43}\n",
      "{'loss': 2.018, 'grad_norm': 3.9599409103393555, 'learning_rate': 4.2571959145775305e-05, 'epoch': 0.45}\n",
      "{'loss': 2.1619, 'grad_norm': 2.59663724899292, 'learning_rate': 4.233983286908078e-05, 'epoch': 0.46}\n",
      "{'loss': 2.1151, 'grad_norm': 3.604987621307373, 'learning_rate': 4.210770659238626e-05, 'epoch': 0.47}\n",
      "{'loss': 2.0989, 'grad_norm': 3.167635440826416, 'learning_rate': 4.1875580315691735e-05, 'epoch': 0.49}\n",
      "{'loss': 2.206, 'grad_norm': 2.614638328552246, 'learning_rate': 4.164345403899722e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2872, 'grad_norm': 3.479656934738159, 'learning_rate': 4.141132776230269e-05, 'epoch': 0.52}\n",
      "{'loss': 2.3207, 'grad_norm': 3.0483031272888184, 'learning_rate': 4.117920148560817e-05, 'epoch': 0.53}\n",
      "{'loss': 2.3153, 'grad_norm': 4.58658504486084, 'learning_rate': 4.094707520891365e-05, 'epoch': 0.54}\n",
      "{'loss': 2.1984, 'grad_norm': 3.3104441165924072, 'learning_rate': 4.071494893221913e-05, 'epoch': 0.56}\n",
      "{'loss': 2.2733, 'grad_norm': 3.3787152767181396, 'learning_rate': 4.04828226555246e-05, 'epoch': 0.57}\n",
      "{'loss': 2.2549, 'grad_norm': 2.8255081176757812, 'learning_rate': 4.0250696378830085e-05, 'epoch': 0.58}\n",
      "{'loss': 1.8934, 'grad_norm': 3.2958171367645264, 'learning_rate': 4.0018570102135566e-05, 'epoch': 0.6}\n",
      "{'loss': 2.32, 'grad_norm': 2.369602680206299, 'learning_rate': 3.978644382544105e-05, 'epoch': 0.61}\n",
      "{'loss': 2.1984, 'grad_norm': 4.475775241851807, 'learning_rate': 3.955431754874652e-05, 'epoch': 0.63}\n",
      "{'loss': 1.9783, 'grad_norm': 2.584275960922241, 'learning_rate': 3.9322191272052003e-05, 'epoch': 0.64}\n",
      "{'loss': 2.1339, 'grad_norm': 3.5608561038970947, 'learning_rate': 3.909006499535748e-05, 'epoch': 0.65}\n",
      "{'loss': 2.0693, 'grad_norm': 2.444566011428833, 'learning_rate': 3.885793871866296e-05, 'epoch': 0.67}\n",
      "{'loss': 2.0648, 'grad_norm': 2.382249355316162, 'learning_rate': 3.8625812441968434e-05, 'epoch': 0.68}\n",
      "{'loss': 2.1668, 'grad_norm': 2.6106150150299072, 'learning_rate': 3.8393686165273915e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2076, 'grad_norm': 2.4056458473205566, 'learning_rate': 3.816155988857939e-05, 'epoch': 0.71}\n",
      "{'loss': 2.1334, 'grad_norm': 3.1784260272979736, 'learning_rate': 3.792943361188487e-05, 'epoch': 0.72}\n",
      "{'loss': 2.1503, 'grad_norm': 2.8308115005493164, 'learning_rate': 3.7697307335190346e-05, 'epoch': 0.74}\n",
      "{'loss': 2.0944, 'grad_norm': 3.062875986099243, 'learning_rate': 3.746518105849583e-05, 'epoch': 0.75}\n",
      "{'loss': 2.0583, 'grad_norm': 4.0444512367248535, 'learning_rate': 3.72330547818013e-05, 'epoch': 0.77}\n",
      "{'loss': 2.0853, 'grad_norm': 2.9924168586730957, 'learning_rate': 3.700092850510678e-05, 'epoch': 0.78}\n",
      "{'loss': 2.3255, 'grad_norm': 3.1218748092651367, 'learning_rate': 3.676880222841226e-05, 'epoch': 0.79}\n",
      "{'loss': 2.2132, 'grad_norm': 2.7614965438842773, 'learning_rate': 3.653667595171773e-05, 'epoch': 0.81}\n",
      "{'loss': 2.0471, 'grad_norm': 2.836404323577881, 'learning_rate': 3.6304549675023214e-05, 'epoch': 0.82}\n",
      "{'loss': 2.1698, 'grad_norm': 2.347529411315918, 'learning_rate': 3.607242339832869e-05, 'epoch': 0.84}\n",
      "{'loss': 2.0642, 'grad_norm': 2.507899761199951, 'learning_rate': 3.584029712163417e-05, 'epoch': 0.85}\n",
      "{'loss': 2.2552, 'grad_norm': 3.5744235515594482, 'learning_rate': 3.5608170844939645e-05, 'epoch': 0.86}\n",
      "{'loss': 2.213, 'grad_norm': 2.338803291320801, 'learning_rate': 3.5376044568245126e-05, 'epoch': 0.88}\n",
      "{'loss': 2.2808, 'grad_norm': 4.5527849197387695, 'learning_rate': 3.51439182915506e-05, 'epoch': 0.89}\n",
      "{'loss': 2.3245, 'grad_norm': 4.201967716217041, 'learning_rate': 3.491179201485608e-05, 'epoch': 0.91}\n",
      "{'loss': 2.2711, 'grad_norm': 2.871767044067383, 'learning_rate': 3.4679665738161556e-05, 'epoch': 0.92}\n",
      "{'loss': 2.1784, 'grad_norm': 2.8703598976135254, 'learning_rate': 3.444753946146704e-05, 'epoch': 0.93}\n",
      "{'loss': 2.2442, 'grad_norm': 3.211496353149414, 'learning_rate': 3.421541318477251e-05, 'epoch': 0.95}\n",
      "{'loss': 1.8642, 'grad_norm': 2.7855024337768555, 'learning_rate': 3.3983286908077994e-05, 'epoch': 0.96}\n",
      "{'loss': 2.0763, 'grad_norm': 2.8681674003601074, 'learning_rate': 3.375116063138347e-05, 'epoch': 0.97}\n",
      "{'loss': 2.1363, 'grad_norm': 3.5139572620391846, 'learning_rate': 3.3519034354688957e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc99d67acf14793a766645a8d39cd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
